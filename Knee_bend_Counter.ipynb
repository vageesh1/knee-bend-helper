{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A colab file for finding the number of time our knees gets bend using mediapipe"
      ],
      "metadata": {
        "id": "cJcpY7-QmjQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRba6MDE4Kuh",
        "outputId": "b06019c5-8a18-4719-f053-30231c95b081"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 33.0 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (22.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.8/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.8/dist-packages (from mediapipe) (3.19.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mediapipe) (1.3.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->mediapipe) (1.15.0)\n",
            "Installing collected packages: flatbuffers, mediapipe\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 22.11.23 which is incompatible.\u001b[0m\n",
            "Successfully installed flatbuffers-22.11.23 mediapipe-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s2HnlJfcmgdM"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import time\n",
        "import math as m\n",
        "import mediapipe as mp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for finding angle"
      ],
      "metadata": {
        "id": "3QQSH41TDlaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For finding the angle we will be using the vector distance approach betwene two co-ordinates "
      ],
      "metadata": {
        "id": "w3qNqWqAEWqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findangle(p0,p1,p2):\n",
        "  v0 = np.array(p0) - np.array(p1)\n",
        "  v1 = np.array(p2) - np.array(p1)\n",
        "\n",
        "  return np.degrees(np.math.atan2(np.linalg.det([v0, v1]), np.dot(v0, v1)))"
      ],
      "metadata": {
        "id": "12PlsskK_zkI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for sending Warning "
      ],
      "metadata": {
        "id": "pU9sqo9tEli-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sendWarning():\n",
        "  print(\"Keep your knee bent\")\n",
        "  pass"
      ],
      "metadata": {
        "id": "5TGj9sFrEiL7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializations"
      ],
      "metadata": {
        "id": "bvtUkv9ZE0lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize frame counters.\n",
        "good_frames = 0\n",
        "bad_frames  = 0"
      ],
      "metadata": {
        "id": "CDE6JvF6Evex"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Font type.\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX"
      ],
      "metadata": {
        "id": "Eemg1TIkE5I6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colors.\n",
        "blue = (255, 127, 0)\n",
        "red = (50, 50, 255)\n",
        "green = (127, 255, 0)\n",
        "dark_blue = (127, 20, 0)\n",
        "light_green = (127, 233, 100)\n",
        "yellow = (0, 255, 255)\n",
        "pink = (255, 0, 255)"
      ],
      "metadata": {
        "id": "JlXCq_uWE7I1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize mediapipe pose class.\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()"
      ],
      "metadata": {
        "id": "z8KailZ1FAQm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video capture and writer "
      ],
      "metadata": {
        "id": "0psot0seHQnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/drive/MyDrive/datasets/KneeBendVideo.mp4'\n",
        "cap = cv2.VideoCapture(file_name)"
      ],
      "metadata": {
        "id": "fMtJx0g7HTex"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "frame_size = (width, height)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')"
      ],
      "metadata": {
        "id": "geAqgT-QHWZf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Video writer.\n",
        "video_output = cv2.VideoWriter('output.mp4', fourcc, fps, frame_size)"
      ],
      "metadata": {
        "id": "uIkI3qc4INMg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capturing the pose"
      ],
      "metadata": {
        "id": "1fZjpH_gIX9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture frames.\n",
        "success, image = cap.read()"
      ],
      "metadata": {
        "id": "imJGOwTFIOwe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get fps.\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)"
      ],
      "metadata": {
        "id": "iNduavs8I0Km"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get height and width of the frame.\n",
        "h, w = image.shape[:2]"
      ],
      "metadata": {
        "id": "2yTUubt1I-eW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the BGR image to RGB.\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ],
      "metadata": {
        "id": "iI_8KCqCJB9G"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the image.\n",
        "keypoints = pose.process(image)"
      ],
      "metadata": {
        "id": "cMz_AKdXJEJN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the image back to BGR.\n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"
      ],
      "metadata": {
        "id": "AUacPX5mJGTI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use lm and lmPose as representative of the following methods.\n",
        "lm = keypoints.pose_landmarks\n",
        "lmPose  = mp_pose.PoseLandmark"
      ],
      "metadata": {
        "id": "o4tG-Fv9JhKq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hip co-ordiantes\n",
        "# right hip.\n",
        "r_hip_x = int(lm.landmark[lmPose.RIGHT_HIP].x * w)\n",
        "r_hip_y = int(lm.landmark[lmPose.RIGHT_HIP].y * h)"
      ],
      "metadata": {
        "id": "PHkED-4KRM4B"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#knee co-ordinates \n",
        "r_knee_x = int(lm.landmark[lmPose.RIGHT_KNEE].x * w)\n",
        "r_knee_y = int(lm.landmark[lmPose.RIGHT_KNEE].y * h)"
      ],
      "metadata": {
        "id": "bVQF9iKRRrbE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#right ankle co-ordinates \n",
        "r_ankle_x = int(lm.landmark[lmPose.RIGHT_ANKLE].x * w)\n",
        "r_ankle_y = int(lm.landmark[lmPose.RIGHT_ANKLE].y * h)"
      ],
      "metadata": {
        "id": "G1Dl1Be7Rxgr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the incliantion"
      ],
      "metadata": {
        "id": "2ctf18oISKOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting the REp for knee"
      ],
      "metadata": {
        "id": "vH6epPtoTYm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Processing..')\n",
        "while cap.isOpened():\n",
        "    # Capture frames.\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "        print(\"Null.Frames\")\n",
        "        break\n",
        "    # Get fps.\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    # Get height and width.\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Convert the BGR image to RGB.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the image.\n",
        "    keypoints = pose.process(image)\n",
        "\n",
        "    # Convert the image back to BGR.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Use lm and lmPose as representative of the following methods.\n",
        "    lm = keypoints.pose_landmarks\n",
        "    lmPose = mp_pose.PoseLandmark\n",
        "\n",
        "    # Acquire the landmark coordinates.\n",
        "    # Once aligned properly, left or right should not be a concern.      \n",
        "    # Right HIP\n",
        "    if lm is not None:\n",
        "      r_hip_x = int(lm.landmark[lmPose.RIGHT_HIP].x * w)\n",
        "      r_hip_y = int(lm.landmark[lmPose.RIGHT_HIP].y * h)\n",
        "      # Right knee\n",
        "      r_knee_x = int(lm.landmark[lmPose.RIGHT_KNEE].x * w)\n",
        "      r_knee_y = int(lm.landmark[lmPose.RIGHT_KNEE].y * h)\n",
        "      # Right Ankle.\n",
        "      r_ankle_x = int(lm.landmark[lmPose.RIGHT_ANKLE].x * w)\n",
        "      r_ankle_y = int(lm.landmark[lmPose.RIGHT_ANKLE].y * h)    \n",
        "\n",
        "\n",
        "    angle= -(findangle([r_hip_x,1-r_hip_y],[r_knee_x,1-r_knee_y],[r_ankle_x,1-r_ankle_y]))\n",
        "\n",
        "    # Put text, Posture and angle inclination.\n",
        "    # Text string for display.\n",
        "    angle_text_string = (f'The angle of knee bending is {angle}')\n",
        "\n",
        "    # Determine whether good posture or bad posture.\n",
        "    # The threshold angles have been set based on intuition.\n",
        "    if (angle<140):\n",
        "      bad_frames = 0\n",
        "      good_frames += 1\n",
        "      good_time = (1 / fps) * good_frames\n",
        "      bad_time =  (1 / fps) * bad_frames\n",
        "      #putting angle on each part\n",
        "      cv2.putText(image, angle_text_string, (10, 30), font, 0.9, light_green, 2)\n",
        "      # Join landmarks.\n",
        "      cv2.line(image, (r_hip_x, r_hip_y), (r_knee_x, r_knee_y), green, 4)\n",
        "      cv2.line(image, (r_knee_x, r_knee_y), (r_ankle_x, r_ankle_y), green, 4)\n",
        "      time_string_good = 'Good Posture Time : ' + str(round(good_time, 1)) + 's'\n",
        "      cv2.putText(image, time_string_good, (10, h - 20), font, 0.9, green, 2)\n",
        "\n",
        "      if good_time>8: \n",
        "        cv2.putText(image,'Now pls rest you knee',(50,50),font,0.9,red,2)\n",
        "\n",
        "      else: \n",
        "        cv2.putText(image,'Keep your knee bend ',(50,50),font,0.9,red,2)\n",
        "\n",
        "      \n",
        "\n",
        "    else: \n",
        "      good_frames = 0\n",
        "      bad_frames += 1\n",
        "      good_time = (1 / fps) * good_frames\n",
        "      bad_time =  (1 / fps) * bad_frames\n",
        "      #putting angle on each part\n",
        "      cv2.putText(image, angle_text_string, (10, 30), font, 0.9, light_green, 2)\n",
        "      \n",
        "      # Join landmarks.\n",
        "      cv2.line(image, (r_hip_x, r_hip_y), (r_knee_x, r_knee_y), green, 4)\n",
        "      cv2.line(image, (r_knee_x, r_knee_y), (r_ankle_x, r_ankle_y), green, 4)\n",
        "      time_string_bad = 'Bad Posture Time : ' + str(round(bad_time, 1)) + 's'\n",
        "      cv2.putText(image, time_string_bad, (10, h - 20), font, 0.9, red, 2)\n",
        "\n",
        "      cv2.putText(image,'Please bend your knee',(50,50),font,0.9,red,2)\n",
        "\n",
        "    # Write frames.\n",
        "    video_output.write(image)\n",
        "print('Finished.')\n",
        "cap.release()\n",
        "video_output.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew25mMMHWsaA",
        "outputId": "663641b7-5c3c-48cd-c1ad-f820654464c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing..\n",
            "Null.Frames\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwLZvlGsG7xG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}